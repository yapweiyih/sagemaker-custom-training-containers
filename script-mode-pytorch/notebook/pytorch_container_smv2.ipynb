{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Create custom container using SageMaker PyTorch Deep Learning Framework\n",
    "\n",
    "Update `role` with your SageMaker role arn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pip 20.2.1 from /Users/yihyap/anaconda3/envs/smv2/lib/python3.8/site-packages/pip (python 3.8)\n"
     ]
    }
   ],
   "source": [
    "!pip --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Account: 342474125894\nRegion: ap-southeast-1\nRole: arn:aws:iam::342474125894:role/service-role/AmazonSageMaker-ExecutionRole-20190405T234154\nS3 Bucket: sagemaker-ap-southeast-1-342474125894\nRepo: sagemaker-training-containers/pytorch-training\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.pytorch import PyTorch\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "ecr_namespace = 'sagemaker-training-containers/'\n",
    "prefix = 'pytorch-training'\n",
    "ecr_repository_name = ecr_namespace + prefix\n",
    "\n",
    "\n",
    "ecr_repository_name = ecr_namespace + prefix\n",
    "role = \"arn:aws:iam::342474125894:role/service-role/AmazonSageMaker-ExecutionRole-20190405T234154\"\n",
    "account_id = role.split(':')[4]\n",
    "region = boto3.Session().region_name\n",
    "sagemaker_session = sagemaker.session.Session()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "print('Account: {}'.format(account_id))\n",
    "print('Region: {}'.format(region))\n",
    "print('Role: {}'.format(role))\n",
    "print('S3 Bucket: {}'.format(bucket))\n",
    "print('Repo: {}'.format(ecr_repository_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build training container\n",
    "\n",
    "Next we will create a script that will build and upload the custom container image into ECR. It has to be in the same region where the job is run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "grade pip             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [5/9] RUN ln -s $(which python3) /usr/local/bin/python          0.0s\n",
      "\u001b[0m\u001b[34m => [6/9] RUN pip3 install --no-cache --upgrade         numpy==1.14.5    314.4s\n",
      "\u001b[0m\u001b[34m => [7/9] RUN pip3 install --no-cache --upgrade     sagemaker-container  134.7s\n",
      "\u001b[0m\u001b[34m => [8/9] COPY code/* /opt/ml/code/                                        0.1s\n",
      "\u001b[0m => exporting to image                                                    11.7s\n",
      " => => exporting layers                                                   11.7s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 463.8s (13/14)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 2.38kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/library/ubuntu:16.04            2.7s\n",
      "\u001b[0m\u001b[34m => [auth] library/ubuntu:pull token for registry-1.docker.io              0.0s\n",
      "\u001b[0m\u001b[34m => [1/9] FROM docker.io/library/ubuntu:16.04@sha256:3355b6e4ba1b12071ba5  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 10.27kB                                       0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/9] RUN apt-get update && apt-get install -y --no-install-re  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [3/9] RUN wget https://www.python.org/ftp/python/3.6.6/Python-  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [4/9] RUN pip3 --no-cache-dir install --upgrade pip             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [5/9] RUN ln -s $(which python3) /usr/local/bin/python          0.0s\n",
      "\u001b[0m\u001b[34m => [6/9] RUN pip3 install --no-cache --upgrade         numpy==1.14.5    314.4s\n",
      "\u001b[0m\u001b[34m => [7/9] RUN pip3 install --no-cache --upgrade     sagemaker-container  134.7s\n",
      "\u001b[0m\u001b[34m => [8/9] COPY code/* /opt/ml/code/                                        0.1s\n",
      "\u001b[0m => exporting to image                                                    11.9s\n",
      " => => exporting layers                                                   11.9s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 464.0s (13/14)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 2.38kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/library/ubuntu:16.04            2.7s\n",
      "\u001b[0m\u001b[34m => [auth] library/ubuntu:pull token for registry-1.docker.io              0.0s\n",
      "\u001b[0m\u001b[34m => [1/9] FROM docker.io/library/ubuntu:16.04@sha256:3355b6e4ba1b12071ba5  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 10.27kB                                       0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/9] RUN apt-get update && apt-get install -y --no-install-re  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [3/9] RUN wget https://www.python.org/ftp/python/3.6.6/Python-  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [4/9] RUN pip3 --no-cache-dir install --upgrade pip             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [5/9] RUN ln -s $(which python3) /usr/local/bin/python          0.0s\n",
      "\u001b[0m\u001b[34m => [6/9] RUN pip3 install --no-cache --upgrade         numpy==1.14.5    314.4s\n",
      "\u001b[0m\u001b[34m => [7/9] RUN pip3 install --no-cache --upgrade     sagemaker-container  134.7s\n",
      "\u001b[0m\u001b[34m => [8/9] COPY code/* /opt/ml/code/                                        0.1s\n",
      "\u001b[0m => exporting to image                                                    12.0s\n",
      " => => exporting layers                                                   12.0s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 464.1s (13/14)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 2.38kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/library/ubuntu:16.04            2.7s\n",
      "\u001b[0m\u001b[34m => [auth] library/ubuntu:pull token for registry-1.docker.io              0.0s\n",
      "\u001b[0m\u001b[34m => [1/9] FROM docker.io/library/ubuntu:16.04@sha256:3355b6e4ba1b12071ba5  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 10.27kB                                       0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/9] RUN apt-get update && apt-get install -y --no-install-re  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [3/9] RUN wget https://www.python.org/ftp/python/3.6.6/Python-  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [4/9] RUN pip3 --no-cache-dir install --upgrade pip             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [5/9] RUN ln -s $(which python3) /usr/local/bin/python          0.0s\n",
      "\u001b[0m\u001b[34m => [6/9] RUN pip3 install --no-cache --upgrade         numpy==1.14.5    314.4s\n",
      "\u001b[0m\u001b[34m => [7/9] RUN pip3 install --no-cache --upgrade     sagemaker-container  134.7s\n",
      "\u001b[0m\u001b[34m => [8/9] COPY code/* /opt/ml/code/                                        0.1s\n",
      "\u001b[0m => exporting to image                                                    12.2s\n",
      " => => exporting layers                                                   12.2s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 464.3s (13/14)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 2.38kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/library/ubuntu:16.04            2.7s\n",
      "\u001b[0m\u001b[34m => [auth] library/ubuntu:pull token for registry-1.docker.io              0.0s\n",
      "\u001b[0m\u001b[34m => [1/9] FROM docker.io/library/ubuntu:16.04@sha256:3355b6e4ba1b12071ba5  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 10.27kB                                       0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/9] RUN apt-get update && apt-get install -y --no-install-re  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [3/9] RUN wget https://www.python.org/ftp/python/3.6.6/Python-  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [4/9] RUN pip3 --no-cache-dir install --upgrade pip             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [5/9] RUN ln -s $(which python3) /usr/local/bin/python          0.0s\n",
      "\u001b[0m\u001b[34m => [6/9] RUN pip3 install --no-cache --upgrade         numpy==1.14.5    314.4s\n",
      "\u001b[0m\u001b[34m => [7/9] RUN pip3 install --no-cache --upgrade     sagemaker-container  134.7s\n",
      "\u001b[0m\u001b[34m => [8/9] COPY code/* /opt/ml/code/                                        0.1s\n",
      "\u001b[0m => exporting to image                                                    12.3s\n",
      " => => exporting layers                                                   12.3s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 464.4s (13/14)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 2.38kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/library/ubuntu:16.04            2.7s\n",
      "\u001b[0m\u001b[34m => [auth] library/ubuntu:pull token for registry-1.docker.io              0.0s\n",
      "\u001b[0m\u001b[34m => [1/9] FROM docker.io/library/ubuntu:16.04@sha256:3355b6e4ba1b12071ba5  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 10.27kB                                       0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/9] RUN apt-get update && apt-get install -y --no-install-re  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [3/9] RUN wget https://www.python.org/ftp/python/3.6.6/Python-  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [4/9] RUN pip3 --no-cache-dir install --upgrade pip             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [5/9] RUN ln -s $(which python3) /usr/local/bin/python          0.0s\n",
      "\u001b[0m\u001b[34m => [6/9] RUN pip3 install --no-cache --upgrade         numpy==1.14.5    314.4s\n",
      "\u001b[0m\u001b[34m => [7/9] RUN pip3 install --no-cache --upgrade     sagemaker-container  134.7s\n",
      "\u001b[0m\u001b[34m => [8/9] COPY code/* /opt/ml/code/                                        0.1s\n",
      "\u001b[0m => exporting to image                                                    12.5s\n",
      " => => exporting layers                                                   12.5s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 464.6s (13/14)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 2.38kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/library/ubuntu:16.04            2.7s\n",
      "\u001b[0m\u001b[34m => [auth] library/ubuntu:pull token for registry-1.docker.io              0.0s\n",
      "\u001b[0m\u001b[34m => [1/9] FROM docker.io/library/ubuntu:16.04@sha256:3355b6e4ba1b12071ba5  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 10.27kB                                       0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/9] RUN apt-get update && apt-get install -y --no-install-re  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [3/9] RUN wget https://www.python.org/ftp/python/3.6.6/Python-  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [4/9] RUN pip3 --no-cache-dir install --upgrade pip             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [5/9] RUN ln -s $(which python3) /usr/local/bin/python          0.0s\n",
      "\u001b[0m\u001b[34m => [6/9] RUN pip3 install --no-cache --upgrade         numpy==1.14.5    314.4s\n",
      "\u001b[0m\u001b[34m => [7/9] RUN pip3 install --no-cache --upgrade     sagemaker-container  134.7s\n",
      "\u001b[0m\u001b[34m => [8/9] COPY code/* /opt/ml/code/                                        0.1s\n",
      "\u001b[0m => exporting to image                                                    12.6s\n",
      " => => exporting layers                                                   12.6s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 464.7s (13/14)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 2.38kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/library/ubuntu:16.04            2.7s\n",
      "\u001b[0m\u001b[34m => [auth] library/ubuntu:pull token for registry-1.docker.io              0.0s\n",
      "\u001b[0m\u001b[34m => [1/9] FROM docker.io/library/ubuntu:16.04@sha256:3355b6e4ba1b12071ba5  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 10.27kB                                       0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/9] RUN apt-get update && apt-get install -y --no-install-re  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [3/9] RUN wget https://www.python.org/ftp/python/3.6.6/Python-  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [4/9] RUN pip3 --no-cache-dir install --upgrade pip             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [5/9] RUN ln -s $(which python3) /usr/local/bin/python          0.0s\n",
      "\u001b[0m\u001b[34m => [6/9] RUN pip3 install --no-cache --upgrade         numpy==1.14.5    314.4s\n",
      "\u001b[0m\u001b[34m => [7/9] RUN pip3 install --no-cache --upgrade     sagemaker-container  134.7s\n",
      "\u001b[0m\u001b[34m => [8/9] COPY code/* /opt/ml/code/                                        0.1s\n",
      "\u001b[0m => exporting to image                                                    12.8s\n",
      " => => exporting layers                                                   12.8s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 464.9s (13/14)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 2.38kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/library/ubuntu:16.04            2.7s\n",
      "\u001b[0m\u001b[34m => [auth] library/ubuntu:pull token for registry-1.docker.io              0.0s\n",
      "\u001b[0m\u001b[34m => [1/9] FROM docker.io/library/ubuntu:16.04@sha256:3355b6e4ba1b12071ba5  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 10.27kB                                       0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/9] RUN apt-get update && apt-get install -y --no-install-re  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [3/9] RUN wget https://www.python.org/ftp/python/3.6.6/Python-  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [4/9] RUN pip3 --no-cache-dir install --upgrade pip             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [5/9] RUN ln -s $(which python3) /usr/local/bin/python          0.0s\n",
      "\u001b[0m\u001b[34m => [6/9] RUN pip3 install --no-cache --upgrade         numpy==1.14.5    314.4s\n",
      "\u001b[0m\u001b[34m => [7/9] RUN pip3 install --no-cache --upgrade     sagemaker-container  134.7s\n",
      "\u001b[0m\u001b[34m => [8/9] COPY code/* /opt/ml/code/                                        0.1s\n",
      "\u001b[0m => exporting to image                                                    12.9s\n",
      " => => exporting layers                                                   12.9s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 465.0s (13/14)                                                     \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 2.38kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/library/ubuntu:16.04            2.7s\n",
      "\u001b[0m\u001b[34m => [auth] library/ubuntu:pull token for registry-1.docker.io              0.0s\n",
      "\u001b[0m\u001b[34m => [1/9] FROM docker.io/library/ubuntu:16.04@sha256:3355b6e4ba1b12071ba5  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 10.27kB                                       0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/9] RUN apt-get update && apt-get install -y --no-install-re  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [3/9] RUN wget https://www.python.org/ftp/python/3.6.6/Python-  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [4/9] RUN pip3 --no-cache-dir install --upgrade pip             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [5/9] RUN ln -s $(which python3) /usr/local/bin/python          0.0s\n",
      "\u001b[0m\u001b[34m => [6/9] RUN pip3 install --no-cache --upgrade         numpy==1.14.5    314.4s\n",
      "\u001b[0m\u001b[34m => [7/9] RUN pip3 install --no-cache --upgrade     sagemaker-container  134.7s\n",
      "\u001b[0m\u001b[34m => [8/9] COPY code/* /opt/ml/code/                                        0.1s\n",
      "\u001b[0m => exporting to image                                                    13.1s\n",
      " => => exporting layers                                                   13.1s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 465.1s (14/14) FINISHED                                            \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 2.38kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/library/ubuntu:16.04            2.7s\n",
      "\u001b[0m\u001b[34m => [auth] library/ubuntu:pull token for registry-1.docker.io              0.0s\n",
      "\u001b[0m\u001b[34m => [1/9] FROM docker.io/library/ubuntu:16.04@sha256:3355b6e4ba1b12071ba5  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 10.27kB                                       0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/9] RUN apt-get update && apt-get install -y --no-install-re  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [3/9] RUN wget https://www.python.org/ftp/python/3.6.6/Python-  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [4/9] RUN pip3 --no-cache-dir install --upgrade pip             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [5/9] RUN ln -s $(which python3) /usr/local/bin/python          0.0s\n",
      "\u001b[0m\u001b[34m => [6/9] RUN pip3 install --no-cache --upgrade         numpy==1.14.5    314.4s\n",
      "\u001b[0m\u001b[34m => [7/9] RUN pip3 install --no-cache --upgrade     sagemaker-container  134.7s\n",
      "\u001b[0m\u001b[34m => [8/9] COPY code/* /opt/ml/code/                                        0.1s\n",
      "\u001b[0m\u001b[34m => exporting to image                                                    13.1s\n",
      "\u001b[0m\u001b[34m => => exporting layers                                                   13.1s\n",
      "\u001b[0m\u001b[34m => => writing image sha256:8ac0297edebba78fc34a90fd5a8fbb75fcb4c03230bae  0.0s\n",
      "\u001b[0m\u001b[34m => => naming to docker.io/sagemaker-training-containers/pytorch-training  0.0s\n",
      "\u001b[0m\u001b[?25h+ docker tag sagemaker-training-containers/pytorch-training 342474125894.dkr.ecr.ap-southeast-1.amazonaws.com/sagemaker-training-containers/pytorch-training:latest\n",
      "+ SERVER=342474125894.dkr.ecr.ap-southeast-1.amazonaws.com\n",
      "+ aws ecr get-login-password\n",
      "+ docker login --username AWS --password-stdin 342474125894.dkr.ecr.ap-southeast-1.amazonaws.com\n",
      "Login Succeeded\n",
      "+ echo '***Create Repo***'\n",
      "***Create Repo***\n",
      "+ aws ecr describe-repositories --repository-names sagemaker-training-containers/pytorch-training\n",
      "{\n",
      "    \"repositories\": [\n",
      "        {\n",
      "            \"repositoryArn\": \"arn:aws:ecr:ap-southeast-1:342474125894:repository/sagemaker-training-containers/pytorch-training\",\n",
      "            \"registryId\": \"342474125894\",\n",
      "            \"repositoryName\": \"sagemaker-training-containers/pytorch-training\",\n",
      "            \"repositoryUri\": \"342474125894.dkr.ecr.ap-southeast-1.amazonaws.com/sagemaker-training-containers/pytorch-training\",\n",
      "            \"createdAt\": \"2020-08-11T19:41:02+08:00\",\n",
      "            \"imageTagMutability\": \"MUTABLE\",\n",
      "            \"imageScanningConfiguration\": {\n",
      "                \"scanOnPush\": false\n",
      "            },\n",
      "            \"encryptionConfiguration\": {\n",
      "                \"encryptionType\": \"AES256\"\n",
      "            }\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "+ echo '***Push to Repo***'\n",
      "***Push to Repo***\n",
      "+ docker push 342474125894.dkr.ecr.ap-southeast-1.amazonaws.com/sagemaker-training-containers/pytorch-training:latest\n",
      "The push refers to repository [342474125894.dkr.ecr.ap-southeast-1.amazonaws.com/sagemaker-training-containers/pytorch-training]\n",
      "\n",
      "\u001b[1B\n",
      "\u001b[1B\n",
      "\u001b[1B\n",
      "\u001b[1B\n",
      "\u001b[1B\n",
      "\u001b[1B\n",
      "\u001b[1B\n",
      "\u001b[1B\n",
      "\u001b[1B\n",
      "\u001b[1B\n",
      "\u001b[9Blatest: digest: sha256:686c8b90b274d760bf3234bf8275584dac639aacf99007a60d188043f1ae9c27 size: 2626\n"
     ]
    }
   ],
   "source": [
    "# ./build_and_push.sh 342474125894 ap-southeast-1 sagemaker-training-containers/pytorch-training\n",
    "! ../scripts/build_and_push.sh $account_id $region $ecr_repository_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ECR training container ARN: 342474125894.dkr.ecr.ap-southeast-1.amazonaws.com/sagemaker-training-containers/pytorch-training:latest\n"
     ]
    }
   ],
   "source": [
    "train_image_uri = '{0}.dkr.ecr.{1}.amazonaws.com/{2}:latest'.format(account_id, region, ecr_repository_name)\n",
    "print('ECR training container ARN: {}'.format(train_image_uri))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The docker image is now pushed to ECR. In the next section, we will show how to train an acoustic classification model using the custom container."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Training on SageMaker PyTorch custom container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0/3500 (73%)] Loss: 0.481552\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 17 [3200/3500 (91%)] Loss: 0.327987\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m \n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Test set: Average loss: 0.4256, Accuracy: 1267/1500 (84%)\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m \n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 18 [0/3500 (0%)] Loss: 0.482572\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 18 [640/3500 (18%)] Loss: 0.483456\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 18 [1280/3500 (36%)] Loss: 0.413253\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 18 [1920/3500 (55%)] Loss: 0.495300\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 18 [2560/3500 (73%)] Loss: 0.277816\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 18 [3200/3500 (91%)] Loss: 0.211211\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m \n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Test set: Average loss: 0.4401, Accuracy: 1230/1500 (82%)\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m \n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 19 [0/3500 (0%)] Loss: 0.264130\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 19 [640/3500 (18%)] Loss: 0.402724\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 19 [1280/3500 (36%)] Loss: 0.386365\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 19 [1920/3500 (55%)] Loss: 0.320335\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 19 [2560/3500 (73%)] Loss: 0.564965\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 19 [3200/3500 (91%)] Loss: 0.329936\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m \n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Test set: Average loss: 0.4450, Accuracy: 1240/1500 (83%)\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m \n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 20 [0/3500 (0%)] Loss: 0.446207\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 20 [640/3500 (18%)] Loss: 0.377637\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 20 [1280/3500 (36%)] Loss: 0.347222\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 20 [1920/3500 (55%)] Loss: 0.438777\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 20 [2560/3500 (73%)] Loss: 0.417890\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 20 [3200/3500 (91%)] Loss: 0.399005\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m \n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Test set: Average loss: 0.4230, Accuracy: 1252/1500 (83%)\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m \n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 21 [0/3500 (0%)] Loss: 0.379959\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 21 [640/3500 (18%)] Loss: 0.247530\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 21 [1280/3500 (36%)] Loss: 0.548076\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 21 [1920/3500 (55%)] Loss: 0.398207\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 21 [2560/3500 (73%)] Loss: 0.254241\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 21 [3200/3500 (91%)] Loss: 0.282625\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m \n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Test set: Average loss: 0.4201, Accuracy: 1247/1500 (83%)\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m \n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 22 [0/3500 (0%)] Loss: 0.390140\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 22 [640/3500 (18%)] Loss: 0.349066\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 22 [1280/3500 (36%)] Loss: 0.321083\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 22 [1920/3500 (55%)] Loss: 0.384971\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 22 [2560/3500 (73%)] Loss: 0.596720\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 22 [3200/3500 (91%)] Loss: 0.481625\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m \n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Test set: Average loss: 0.4194, Accuracy: 1250/1500 (83%)\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m \n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 23 [0/3500 (0%)] Loss: 0.364492\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 23 [640/3500 (18%)] Loss: 0.310967\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 23 [1280/3500 (36%)] Loss: 0.344643\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 23 [1920/3500 (55%)] Loss: 0.245229\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 23 [2560/3500 (73%)] Loss: 0.343620\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 23 [3200/3500 (91%)] Loss: 0.570149\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m \n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Test set: Average loss: 0.4262, Accuracy: 1259/1500 (84%)\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m \n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 24 [0/3500 (0%)] Loss: 0.350036\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 24 [640/3500 (18%)] Loss: 0.581496\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 24 [1280/3500 (36%)] Loss: 0.329318\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 24 [1920/3500 (55%)] Loss: 0.361663\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 24 [2560/3500 (73%)] Loss: 0.264038\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 24 [3200/3500 (91%)] Loss: 0.312838\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m \n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Test set: Average loss: 0.4273, Accuracy: 1266/1500 (84%)\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m \n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 25 [0/3500 (0%)] Loss: 0.458946\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 25 [640/3500 (18%)] Loss: 0.398937\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 25 [1280/3500 (36%)] Loss: 0.319506\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 25 [1920/3500 (55%)] Loss: 0.411641\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 25 [2560/3500 (73%)] Loss: 0.464415\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 25 [3200/3500 (91%)] Loss: 0.506654\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m \n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Test set: Average loss: 0.4223, Accuracy: 1243/1500 (83%)\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m \n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 26 [0/3500 (0%)] Loss: 0.480214\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 26 [640/3500 (18%)] Loss: 0.376852\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 26 [1280/3500 (36%)] Loss: 0.494405\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 26 [1920/3500 (55%)] Loss: 0.242196\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 26 [2560/3500 (73%)] Loss: 0.495914\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 26 [3200/3500 (91%)] Loss: 0.380194\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m \n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Test set: Average loss: 0.4251, Accuracy: 1252/1500 (83%)\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m \n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 27 [0/3500 (0%)] Loss: 0.466493\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 27 [640/3500 (18%)] Loss: 0.262509\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 27 [1280/3500 (36%)] Loss: 0.360801\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 27 [1920/3500 (55%)] Loss: 0.454384\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 27 [2560/3500 (73%)] Loss: 0.553902\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 27 [3200/3500 (91%)] Loss: 0.444870\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m \n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Test set: Average loss: 0.4253, Accuracy: 1246/1500 (83%)\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m \n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 28 [0/3500 (0%)] Loss: 0.409497\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 28 [640/3500 (18%)] Loss: 0.593244\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 28 [1280/3500 (36%)] Loss: 0.446979\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 28 [1920/3500 (55%)] Loss: 0.495782\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 28 [2560/3500 (73%)] Loss: 0.645534\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 28 [3200/3500 (91%)] Loss: 0.342025\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m \n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Test set: Average loss: 0.4344, Accuracy: 1244/1500 (83%)\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m \n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 29 [0/3500 (0%)] Loss: 0.332243\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 29 [640/3500 (18%)] Loss: 0.654251\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 29 [1280/3500 (36%)] Loss: 0.351071\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 29 [1920/3500 (55%)] Loss: 0.340962\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 29 [2560/3500 (73%)] Loss: 0.292413\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 29 [3200/3500 (91%)] Loss: 0.316809\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m \n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Test set: Average loss: 0.4367, Accuracy: 1239/1500 (83%)\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m \n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 30 [0/3500 (0%)] Loss: 0.335877\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 30 [640/3500 (18%)] Loss: 0.333614\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 30 [1280/3500 (36%)] Loss: 0.478741\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 30 [1920/3500 (55%)] Loss: 0.450580\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 30 [2560/3500 (73%)] Loss: 0.335783\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 30 [3200/3500 (91%)] Loss: 0.348868\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m \n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Test set: Average loss: 0.4286, Accuracy: 1238/1500 (83%)\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m \n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 31 [0/3500 (0%)] Loss: 0.396271\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 31 [640/3500 (18%)] Loss: 0.406700\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 31 [1280/3500 (36%)] Loss: 0.293778\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 31 [1920/3500 (55%)] Loss: 0.308460\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 31 [2560/3500 (73%)] Loss: 0.324566\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 31 [3200/3500 (91%)] Loss: 0.544113\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m \n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Test set: Average loss: 0.4404, Accuracy: 1244/1500 (83%)\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m \n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 32 [0/3500 (0%)] Loss: 0.423359\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 32 [640/3500 (18%)] Loss: 0.589273\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 32 [1280/3500 (36%)] Loss: 0.258480\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 32 [1920/3500 (55%)] Loss: 0.396374\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 32 [2560/3500 (73%)] Loss: 0.390371\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 32 [3200/3500 (91%)] Loss: 0.295323\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m \n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Test set: Average loss: 0.4324, Accuracy: 1250/1500 (83%)\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m \n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 33 [0/3500 (0%)] Loss: 0.393425\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 33 [640/3500 (18%)] Loss: 0.522149\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 33 [1280/3500 (36%)] Loss: 0.465824\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 33 [1920/3500 (55%)] Loss: 0.368218\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 33 [2560/3500 (73%)] Loss: 0.333201\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 33 [3200/3500 (91%)] Loss: 0.590629\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m \n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Test set: Average loss: 0.4509, Accuracy: 1250/1500 (83%)\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m \n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 34 [0/3500 (0%)] Loss: 0.520165\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 34 [640/3500 (18%)] Loss: 0.417074\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 34 [1280/3500 (36%)] Loss: 0.431378\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 34 [1920/3500 (55%)] Loss: 0.487566\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 34 [2560/3500 (73%)] Loss: 0.299634\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 34 [3200/3500 (91%)] Loss: 0.462624\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m \n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Test set: Average loss: 0.4575, Accuracy: 1239/1500 (83%)\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m \n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 35 [0/3500 (0%)] Loss: 0.385870\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 35 [640/3500 (18%)] Loss: 0.390680\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 35 [1280/3500 (36%)] Loss: 0.207709\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 35 [1920/3500 (55%)] Loss: 0.336492\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 35 [2560/3500 (73%)] Loss: 0.395977\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 35 [3200/3500 (91%)] Loss: 0.519383\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m \n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Test set: Average loss: 0.4266, Accuracy: 1245/1500 (83%)\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m \n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 36 [0/3500 (0%)] Loss: 0.559211\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 36 [640/3500 (18%)] Loss: 0.286058\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 36 [1280/3500 (36%)] Loss: 0.530083\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 36 [1920/3500 (55%)] Loss: 0.474187\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 36 [2560/3500 (73%)] Loss: 0.378238\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 36 [3200/3500 (91%)] Loss: 0.408914\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m \n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Test set: Average loss: 0.4269, Accuracy: 1248/1500 (83%)\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m \n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 37 [0/3500 (0%)] Loss: 0.494479\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 37 [640/3500 (18%)] Loss: 0.434816\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 37 [1280/3500 (36%)] Loss: 0.455655\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 37 [1920/3500 (55%)] Loss: 0.465199\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 37 [2560/3500 (73%)] Loss: 0.426496\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 37 [3200/3500 (91%)] Loss: 0.401213\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m \n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Test set: Average loss: 0.4261, Accuracy: 1254/1500 (84%)\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m \n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 38 [0/3500 (0%)] Loss: 0.353980\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 38 [640/3500 (18%)] Loss: 0.273414\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 38 [1280/3500 (36%)] Loss: 0.341129\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 38 [1920/3500 (55%)] Loss: 0.603516\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 38 [2560/3500 (73%)] Loss: 0.352882\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 38 [3200/3500 (91%)] Loss: 0.620483\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m \n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Test set: Average loss: 0.4219, Accuracy: 1261/1500 (84%)\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m \n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 39 [0/3500 (0%)] Loss: 0.274017\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 39 [640/3500 (18%)] Loss: 0.261044\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 39 [1280/3500 (36%)] Loss: 0.312133\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 39 [1920/3500 (55%)] Loss: 0.389925\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 39 [2560/3500 (73%)] Loss: 0.381446\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 39 [3200/3500 (91%)] Loss: 0.359029\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m \n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Test set: Average loss: 0.4332, Accuracy: 1249/1500 (83%)\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m \n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 40 [0/3500 (0%)] Loss: 0.386245\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 40 [640/3500 (18%)] Loss: 0.320299\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 40 [1280/3500 (36%)] Loss: 0.280539\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 40 [1920/3500 (55%)] Loss: 0.240351\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 40 [2560/3500 (73%)] Loss: 0.401280\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 40 [3200/3500 (91%)] Loss: 0.398922\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m \n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Test set: Average loss: 0.4451, Accuracy: 1249/1500 (83%)\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m \n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 41 [0/3500 (0%)] Loss: 0.399067\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 41 [640/3500 (18%)] Loss: 0.401601\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 41 [1280/3500 (36%)] Loss: 0.597290\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 41 [1920/3500 (55%)] Loss: 0.272278\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 41 [2560/3500 (73%)] Loss: 0.542560\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 41 [3200/3500 (91%)] Loss: 0.530195\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m \n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Test set: Average loss: 0.4218, Accuracy: 1264/1500 (84%)\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m \n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 42 [0/3500 (0%)] Loss: 0.472464\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 42 [640/3500 (18%)] Loss: 0.371453\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 42 [1280/3500 (36%)] Loss: 0.379603\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 42 [1920/3500 (55%)] Loss: 0.484228\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 42 [2560/3500 (73%)] Loss: 0.461352\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 42 [3200/3500 (91%)] Loss: 0.437572\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m \n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Test set: Average loss: 0.4227, Accuracy: 1247/1500 (83%)\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m \n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 43 [0/3500 (0%)] Loss: 0.316271\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 43 [640/3500 (18%)] Loss: 0.297293\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 43 [1280/3500 (36%)] Loss: 0.412479\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 43 [1920/3500 (55%)] Loss: 0.321667\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 43 [2560/3500 (73%)] Loss: 0.311728\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 43 [3200/3500 (91%)] Loss: 0.455322\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m \n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Test set: Average loss: 0.4238, Accuracy: 1255/1500 (84%)\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m \n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 44 [0/3500 (0%)] Loss: 0.315569\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 44 [640/3500 (18%)] Loss: 0.218039\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 44 [1280/3500 (36%)] Loss: 0.408284\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 44 [1920/3500 (55%)] Loss: 0.392952\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 44 [2560/3500 (73%)] Loss: 0.590793\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 44 [3200/3500 (91%)] Loss: 0.363226\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m \n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Test set: Average loss: 0.4220, Accuracy: 1263/1500 (84%)\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m \n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 45 [0/3500 (0%)] Loss: 0.299029\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 45 [640/3500 (18%)] Loss: 0.545614\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 45 [1280/3500 (36%)] Loss: 0.277355\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 45 [1920/3500 (55%)] Loss: 0.433069\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 45 [2560/3500 (73%)] Loss: 0.376519\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 45 [3200/3500 (91%)] Loss: 0.564186\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m \n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Test set: Average loss: 0.4493, Accuracy: 1234/1500 (82%)\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m \n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 46 [0/3500 (0%)] Loss: 0.403701\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 46 [640/3500 (18%)] Loss: 0.333400\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 46 [1280/3500 (36%)] Loss: 0.357200\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 46 [1920/3500 (55%)] Loss: 0.449678\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 46 [2560/3500 (73%)] Loss: 0.281702\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 46 [3200/3500 (91%)] Loss: 0.447833\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m \n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Test set: Average loss: 0.4700, Accuracy: 1248/1500 (83%)\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m \n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 47 [0/3500 (0%)] Loss: 0.421688\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 47 [640/3500 (18%)] Loss: 0.277793\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 47 [1280/3500 (36%)] Loss: 0.457946\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 47 [1920/3500 (55%)] Loss: 0.571572\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 47 [2560/3500 (73%)] Loss: 0.487221\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 47 [3200/3500 (91%)] Loss: 0.326367\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m \n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Test set: Average loss: 0.4190, Accuracy: 1261/1500 (84%)\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m \n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 48 [0/3500 (0%)] Loss: 0.308259\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 48 [640/3500 (18%)] Loss: 0.415527\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 48 [1280/3500 (36%)] Loss: 0.387841\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 48 [1920/3500 (55%)] Loss: 0.433055\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 48 [2560/3500 (73%)] Loss: 0.510180\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 48 [3200/3500 (91%)] Loss: 0.409591\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m \n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Test set: Average loss: 0.4520, Accuracy: 1232/1500 (82%)\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m \n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 49 [0/3500 (0%)] Loss: 0.352283\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 49 [640/3500 (18%)] Loss: 0.460601\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 49 [1280/3500 (36%)] Loss: 0.559369\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 49 [1920/3500 (55%)] Loss: 0.449964\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 49 [2560/3500 (73%)] Loss: 0.301321\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 49 [3200/3500 (91%)] Loss: 0.327856\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m \n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Test set: Average loss: 0.4271, Accuracy: 1243/1500 (83%)\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m \n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 50 [0/3500 (0%)] Loss: 0.399802\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 50 [640/3500 (18%)] Loss: 0.276581\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 50 [1280/3500 (36%)] Loss: 0.324120\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 50 [1920/3500 (55%)] Loss: 0.371728\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 50 [2560/3500 (73%)] Loss: 0.262599\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Train Epoch: 50 [3200/3500 (91%)] Loss: 0.448728\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m \n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m Test set: Average loss: 0.4494, Accuracy: 1242/1500 (83%)\n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m \n",
      "\u001b[36malgo-1-1rdjw_1  |\u001b[0m 2021-01-04 08:34:08,556 sagemaker-containers INFO     Reporting training SUCCESS\n",
      "\u001b[36mtmpweljt061_algo-1-1rdjw_1 exited with code 0\n",
      "\u001b[0mAborting on container exit...\n",
      "===== Job Complete =====\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import json\n",
    "\n",
    "hyperparameters = {\n",
    "    \"seed\": \"1\",\n",
    "    \"epochs\": 50,\n",
    "}\n",
    "\n",
    "est = sagemaker.estimator.Estimator(train_image_uri,\n",
    "                                    role,\n",
    "                                    instance_count=1, \n",
    "                                    instance_type='local', # we use local mode\n",
    "                                    #instance_type='ml.m5.xlarge',\n",
    "                                    base_job_name=prefix,\n",
    "                                    hyperparameters=hyperparameters)\n",
    "\n",
    "\n",
    "est.fit()\n",
    "\n",
    "#train_config = sagemaker.inputs.TrainingInput('s3://{0}/{1}/train/'.format(bucket, prefix), content_type='text/csv')\n",
    "#val_config = sagemaker.inputs.TrainingInput('s3://{0}/{1}/val/'.format(bucket, prefix), content_type='text/csv')\n",
    "#est.fit({'train': train_config, 'validation': val_config })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve model location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-ap-southeast-1-342474125894/pytorch-training-2020-08-11-12-33-56-086/output/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "model_location = est.model_data\n",
    "print(model_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Inference\n",
    "\n",
    "For inference, we will use default inference image. Mandatory `model_fn` is implemented in `inference.py`. PyTorchModel is used to deploy custom model that we trained previously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------!"
     ]
    }
   ],
   "source": [
    "from sagemaker.pytorch import PyTorchModel\n",
    "\n",
    "pytorch_model = PyTorchModel(model_data=\"s3://sagemaker-ap-southeast-1-342474125894/pytorch-training-2020-08-11-15-05-07-606/output/model.tar.gz\", \n",
    "                             role=role, \n",
    "                             entry_point='inference.py',\n",
    "                             source_dir='../docker/code',\n",
    "                             py_version='py3',\n",
    "                             framework_version='1.5',\n",
    "                            )\n",
    "predictor = pytorch_model.deploy(initial_instance_count=1, instance_type='ml.m5.xlarge', wait=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_model.endpoint_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch.model import PyTorchPredictor\n",
    "\n",
    "endpoint_name = \"pytorch-inference-2020-08-12-08-52-57-488\"\n",
    "\n",
    "# The PyTorch model uses a npy serializer and deserializer by default\n",
    "predictor = PyTorchPredictor(endpoint_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-9.78976059e+00 -8.42716694e+00 -2.74858845e-04]\n",
      " [-1.21343966e+01 -1.20159941e+01 -1.14440263e-05]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "payload = torch.tensor(np.array([[1,2,3,4,5],[2,3,4,5,6]]), dtype=torch.float)\n",
    "response = predictor.predict(payload)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Optional Cleanup\n",
    "\n",
    "When you're done with the endpoint, you should clean it up.\n",
    "\n",
    "All of the training jobs, models and endpoints we created can be viewed through the SageMaker console of your AWS account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch model test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_features):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_features, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        #x = x.reshape(-1,3)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-6.4697e+00, -6.5448e+00, -2.9917e-03],\n",
       "        [-7.9158e+00, -8.4261e+00, -5.8419e-04]], grad_fn=<LogSoftmaxBackward>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "payload = torch.tensor(np.array([[1,2,3,4,5],[2,3,4,5,6]]), dtype=torch.float)\n",
    "model(payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "\n",
    "\n",
    "X, Y = make_classification(\n",
    "    n_samples=100,\n",
    "    n_features=5,\n",
    "    n_redundant=0,\n",
    "    n_informative=2,\n",
    "    n_clusters_per_class=1,\n",
    "    n_classes=3,\n",
    ")\n",
    "\n",
    "features = torch.FloatTensor(X[0])\n",
    "labels = torch.LongTensor(Y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.60699847, -0.25228405, -0.76545418,  1.23142814,  0.68585389])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.6070, -0.2523, -0.7655,  1.2314,  0.6859])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.FloatTensor(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3977854284320629293])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.LongTensor(Y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4503928797958963200,                   8])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.LongTensor(Y[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(Y[0], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6, 0, 0])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit ('3.8.6')",
   "metadata": {
    "interpreter": {
     "hash": "185eabfcdf4df50349e20bee16b2a0b255a9875bb276c77f7747bf074186d73c"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}