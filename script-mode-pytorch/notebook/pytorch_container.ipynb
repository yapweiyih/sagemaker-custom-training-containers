{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Create custom container using SageMaker PyTorch Deep Learning Framework\n",
    "\n",
    "Update `role` with your SageMaker role arn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pip 20.1 from /Users/yihyap/anaconda3/envs/sandbox36/lib/python3.6/site-packages/pip (python 3.6)\n"
     ]
    }
   ],
   "source": [
    "!pip --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Account: 342474125894\n",
      "Region: ap-southeast-1\n",
      "Role: arn:aws:iam::342474125894:role/service-role/AmazonSageMaker-ExecutionRole-20190405T234154\n",
      "S3 Bucket: sagemaker-ap-southeast-1-342474125894\n",
      "Repo: sagemaker-training-containers/pytorch-training\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.pytorch import PyTorch\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "ecr_namespace = 'sagemaker-training-containers/'\n",
    "prefix = 'pytorch-training'\n",
    "ecr_repository_name = ecr_namespace + prefix\n",
    "\n",
    "\n",
    "ecr_repository_name = ecr_namespace + prefix\n",
    "role = \"arn:aws:iam::342474125894:role/service-role/AmazonSageMaker-ExecutionRole-20190405T234154\"\n",
    "account_id = role.split(':')[4]\n",
    "region = boto3.Session().region_name\n",
    "sagemaker_session = sagemaker.session.Session()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "print('Account: {}'.format(account_id))\n",
    "print('Region: {}'.format(region))\n",
    "print('Role: {}'.format(role))\n",
    "print('S3 Bucket: {}'.format(bucket))\n",
    "print('Repo: {}'.format(ecr_repository_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build training container\n",
    "\n",
    "Next we will create a script that will build and upload the custom container image into ECR. It has to be in the same region where the job is run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  18.43kB\n",
      "Step 1/16 : FROM ubuntu:16.04\n",
      " ---> 13c9f1285025\n",
      "Step 2/16 : LABEL maintainer=\"Giuseppe A. Porcelli\"\n",
      " ---> Using cache\n",
      " ---> 6bbf3d07c68d\n",
      "Step 3/16 : ARG PYTHON=python3\n",
      " ---> Using cache\n",
      " ---> 8e254b9ef0a0\n",
      "Step 4/16 : ARG PYTHON_PIP=python3-pip\n",
      " ---> Using cache\n",
      " ---> 84c928b11bb3\n",
      "Step 5/16 : ARG PIP=pip3\n",
      " ---> Using cache\n",
      " ---> 65e780b1f9d7\n",
      "Step 6/16 : ARG PYTHON_VERSION=3.6.6\n",
      " ---> Using cache\n",
      " ---> 03bab72f170e\n",
      "Step 7/16 : RUN apt-get update && apt-get install -y --no-install-recommends software-properties-common &&     add-apt-repository ppa:deadsnakes/ppa -y &&     apt-get update && apt-get install -y --no-install-recommends         build-essential         ca-certificates         curl         wget         git         libopencv-dev         openssh-client         openssh-server         vim         zlib1g-dev &&     rm -rf /var/lib/apt/lists/*\n",
      " ---> Using cache\n",
      " ---> 0b3f66ca4c73\n",
      "Step 8/16 : RUN wget https://www.python.org/ftp/python/$PYTHON_VERSION/Python-$PYTHON_VERSION.tgz &&         tar -xvf Python-$PYTHON_VERSION.tgz && cd Python-$PYTHON_VERSION &&         ./configure && make && make install &&         apt-get update && apt-get install -y --no-install-recommends libreadline-gplv2-dev libncursesw5-dev libssl-dev libsqlite3-dev tk-dev libgdbm-dev libc6-dev libbz2-dev &&         make && make install && rm -rf ../Python-$PYTHON_VERSION* &&         ln -s /usr/local/bin/pip3 /usr/bin/pip\n",
      " ---> Using cache\n",
      " ---> da24d9684dbd\n",
      "Step 9/16 : RUN ${PIP} --no-cache-dir install --upgrade pip\n",
      " ---> Using cache\n",
      " ---> a7e0f5c77b12\n",
      "Step 10/16 : RUN ln -s $(which ${PYTHON}) /usr/local/bin/python\n",
      " ---> Using cache\n",
      " ---> 9970f3a50688\n",
      "Step 11/16 : WORKDIR /\n",
      " ---> Using cache\n",
      " ---> 7185a26a84ec\n",
      "Step 12/16 : RUN ${PIP} install --no-cache --upgrade         numpy==1.14.5         pandas==0.24.1         scikit-learn==0.20.3         requests==2.21.0         scipy==1.2.1         torch         torchaudio\n",
      " ---> Using cache\n",
      " ---> b994da0ad189\n",
      "Step 13/16 : ENV PYTHONDONTWRITEBYTECODE=1     PYTHONUNBUFFERED=1     LD_LIBRARY_PATH=\"${LD_LIBRARY_PATH}:/usr/local/lib\"     PYTHONIOENCODING=UTF-8     LANG=C.UTF-8     LC_ALL=C.UTF-8\n",
      " ---> Using cache\n",
      " ---> 534c9458df19\n",
      "Step 14/16 : RUN ${PIP} install --no-cache --upgrade     sagemaker-containers\n",
      " ---> Using cache\n",
      " ---> 8a9b3ea0d6eb\n",
      "Step 15/16 : COPY code/* /opt/ml/code/\n",
      " ---> d127a0a1e173\n",
      "Step 16/16 : ENV SAGEMAKER_PROGRAM train.py\n",
      " ---> Running in 8254f06ea793\n",
      "Removing intermediate container 8254f06ea793\n",
      " ---> 3f6b3be0891c\n",
      "Successfully built 3f6b3be0891c\n",
      "Successfully tagged sagemaker-training-containers/pytorch-training:latest\n",
      "Login Succeeded\n",
      "{\n",
      "    \"repositories\": [\n",
      "        {\n",
      "            \"repositoryArn\": \"arn:aws:ecr:ap-southeast-1:342474125894:repository/sagemaker-training-containers/pytorch-training\",\n",
      "            \"registryId\": \"342474125894\",\n",
      "            \"repositoryName\": \"sagemaker-training-containers/pytorch-training\",\n",
      "            \"repositoryUri\": \"342474125894.dkr.ecr.ap-southeast-1.amazonaws.com/sagemaker-training-containers/pytorch-training\",\n",
      "            \"createdAt\": 1597146062.0,\n",
      "            \"imageTagMutability\": \"MUTABLE\",\n",
      "            \"imageScanningConfiguration\": {\n",
      "                \"scanOnPush\": false\n",
      "            }\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "The push refers to repository [342474125894.dkr.ecr.ap-southeast-1.amazonaws.com/sagemaker-training-containers/pytorch-training]\n",
      "\n",
      "\u001b[1B2d779525: Preparing \n",
      "\u001b[1B98b4e3f3: Preparing \n",
      "\u001b[1Bc7c70141: Preparing \n",
      "\u001b[1B4a5b1e5b: Preparing \n",
      "\u001b[1B087edcda: Preparing \n",
      "\u001b[1B0ce8b97f: Preparing \n",
      "\u001b[1Bef1fd00d: Preparing \n",
      "\u001b[1Bf22d44f3: Preparing \n",
      "\u001b[1B6f329a25: Preparing \n",
      "\u001b[1B7de5faec: Preparing \n",
      "\u001b[11Bd779525: Pushed lready exists 3kBA\u001b[2K\u001b[11A\u001b[2K\u001b[6A\u001b[2K\u001b[5A\u001b[2K\u001b[4A\u001b[2K\u001b[1A\u001b[2K\u001b[11A\u001b[2Klatest: digest: sha256:68f51db8cfdd9138573b0e7c496fd73bf082f2c804947447933c24f0aee9aaed size: 2626\n"
     ]
    }
   ],
   "source": [
    "# ./build_and_push.sh 342474125894 ap-southeast-1 sagemaker-training-containers/pytorch-training\n",
    "! ../scripts/build_and_push.sh $account_id $region $ecr_repository_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ECR training container ARN: 342474125894.dkr.ecr.ap-southeast-1.amazonaws.com/sagemaker-training-containers/pytorch-training:latest\n"
     ]
    }
   ],
   "source": [
    "train_image_uri = '{0}.dkr.ecr.{1}.amazonaws.com/{2}:latest'.format(account_id, region, ecr_repository_name)\n",
    "print('ECR training container ARN: {}'.format(train_image_uri))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The docker image is now pushed to ECR. In the next section, we will show how to train an acoustic classification model using the custom container."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Training on SageMaker PyTorch custom container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-08-11 15:04:48 Starting - Starting the training job...\n",
      "2020-08-11 15:04:50 Starting - Launching requested ML instances......\n",
      "2020-08-11 15:06:17 Starting - Preparing the instances for training......\n",
      "2020-08-11 15:07:11 Downloading - Downloading input data\n",
      "2020-08-11 15:07:11 Training - Downloading the training image......\n",
      "2020-08-11 15:08:16 Uploading - Uploading generated training model.\u001b[34m2020-08-11 15:08:11,128 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-08-11 15:08:11,148 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-08-11 15:08:11,161 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-08-11 15:08:11,172 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": null,\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"seed\": 1,\n",
      "        \"epochs\": 50\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"pytorch-training-2020-08-11-15-05-07-606\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"/opt/ml/code\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":50,\"seed\":1}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=/opt/ml/code\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"framework_module\":null,\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":50,\"seed\":1},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pytorch-training-2020-08-11-15-05-07-606\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"/opt/ml/code\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"50\",\"--seed\",\"1\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_HP_SEED=1\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=50\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/usr/local/bin:/usr/local/lib/python36.zip:/usr/local/lib/python3.6:/usr/local/lib/python3.6/lib-dynload:/usr/local/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/usr/local/bin/python3.6 train.py --epochs 50 --seed 1\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mNamespace(batch_size=64, epochs=50, log_interval=10, lr=0.1, model_dir='/opt/ml/model', save_model=True, seed=1, train=None, validation=None)\n",
      "\u001b[0m\n",
      "\u001b[34mList of files in train channel: \u001b[0m\n",
      "\u001b[34mSM_CHANNEL is not set\n",
      "\u001b[0m\n",
      "\u001b[34mList of files in validation channel: \u001b[0m\n",
      "\u001b[34mSM_CHANNEL is not set\u001b[0m\n",
      "\u001b[34mDevice: cpu\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [0/700 (0%)]#011Loss: 1.276576\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [600/700 (91%)]#011Loss: 0.735005\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [0/700 (0%)]#011Loss: 0.530222\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [600/700 (91%)]#011Loss: 0.539475\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [0/700 (0%)]#011Loss: 0.540241\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [600/700 (91%)]#011Loss: 0.439935\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [0/700 (0%)]#011Loss: 0.458141\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [600/700 (91%)]#011Loss: 0.443447\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [0/700 (0%)]#011Loss: 0.448378\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [600/700 (91%)]#011Loss: 0.337838\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [0/700 (0%)]#011Loss: 0.313201\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [600/700 (91%)]#011Loss: 0.508084\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [0/700 (0%)]#011Loss: 0.532282\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [600/700 (91%)]#011Loss: 0.445023\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [0/700 (0%)]#011Loss: 0.366877\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [600/700 (91%)]#011Loss: 0.538606\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [0/700 (0%)]#011Loss: 0.327641\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [600/700 (91%)]#011Loss: 0.527528\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [0/700 (0%)]#011Loss: 0.393070\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [600/700 (91%)]#011Loss: 0.557823\u001b[0m\n",
      "\u001b[34mTrain Epoch: 11 [0/700 (0%)]#011Loss: 0.315303\u001b[0m\n",
      "\u001b[34mTrain Epoch: 11 [600/700 (91%)]#011Loss: 0.401823\u001b[0m\n",
      "\u001b[34mTrain Epoch: 12 [0/700 (0%)]#011Loss: 0.697951\u001b[0m\n",
      "\u001b[34mTrain Epoch: 12 [600/700 (91%)]#011Loss: 0.416821\u001b[0m\n",
      "\u001b[34mTrain Epoch: 13 [0/700 (0%)]#011Loss: 0.468224\u001b[0m\n",
      "\u001b[34mTrain Epoch: 13 [600/700 (91%)]#011Loss: 0.431156\u001b[0m\n",
      "\u001b[34mTrain Epoch: 14 [0/700 (0%)]#011Loss: 0.528336\u001b[0m\n",
      "\u001b[34mTrain Epoch: 14 [600/700 (91%)]#011Loss: 0.489029\u001b[0m\n",
      "\u001b[34mTrain Epoch: 15 [0/700 (0%)]#011Loss: 0.373482\u001b[0m\n",
      "\u001b[34mTrain Epoch: 15 [600/700 (91%)]#011Loss: 0.735736\u001b[0m\n",
      "\u001b[34mTrain Epoch: 16 [0/700 (0%)]#011Loss: 0.303267\u001b[0m\n",
      "\u001b[34mTrain Epoch: 16 [600/700 (91%)]#011Loss: 0.542629\u001b[0m\n",
      "\u001b[34mTrain Epoch: 17 [0/700 (0%)]#011Loss: 0.315380\u001b[0m\n",
      "\u001b[34mTrain Epoch: 17 [600/700 (91%)]#011Loss: 0.450438\u001b[0m\n",
      "\u001b[34mTrain Epoch: 18 [0/700 (0%)]#011Loss: 0.541679\u001b[0m\n",
      "\u001b[34mTrain Epoch: 18 [600/700 (91%)]#011Loss: 0.282461\u001b[0m\n",
      "\u001b[34mTrain Epoch: 19 [0/700 (0%)]#011Loss: 0.502326\u001b[0m\n",
      "\u001b[34mTrain Epoch: 19 [600/700 (91%)]#011Loss: 0.426536\u001b[0m\n",
      "\u001b[34mTrain Epoch: 20 [0/700 (0%)]#011Loss: 0.443382\u001b[0m\n",
      "\u001b[34mTrain Epoch: 20 [600/700 (91%)]#011Loss: 0.505673\u001b[0m\n",
      "\u001b[34mTrain Epoch: 21 [0/700 (0%)]#011Loss: 0.436079\u001b[0m\n",
      "\u001b[34mTrain Epoch: 21 [600/700 (91%)]#011Loss: 0.455380\u001b[0m\n",
      "\u001b[34mTrain Epoch: 22 [0/700 (0%)]#011Loss: 0.438071\u001b[0m\n",
      "\u001b[34mTrain Epoch: 22 [600/700 (91%)]#011Loss: 0.586760\u001b[0m\n",
      "\u001b[34mTrain Epoch: 23 [0/700 (0%)]#011Loss: 0.401094\u001b[0m\n",
      "\u001b[34mTrain Epoch: 23 [600/700 (91%)]#011Loss: 0.486986\u001b[0m\n",
      "\u001b[34mTrain Epoch: 24 [0/700 (0%)]#011Loss: 0.326482\u001b[0m\n",
      "\u001b[34mTrain Epoch: 24 [600/700 (91%)]#011Loss: 0.659913\u001b[0m\n",
      "\u001b[34mTrain Epoch: 25 [0/700 (0%)]#011Loss: 0.533464\u001b[0m\n",
      "\u001b[34mTrain Epoch: 25 [600/700 (91%)]#011Loss: 0.536423\u001b[0m\n",
      "\u001b[34mTrain Epoch: 26 [0/700 (0%)]#011Loss: 0.620597\u001b[0m\n",
      "\u001b[34mTrain Epoch: 26 [600/700 (91%)]#011Loss: 0.535674\u001b[0m\n",
      "\u001b[34mTrain Epoch: 27 [0/700 (0%)]#011Loss: 0.540131\u001b[0m\n",
      "\u001b[34mTrain Epoch: 27 [600/700 (91%)]#011Loss: 0.431616\u001b[0m\n",
      "\u001b[34mTrain Epoch: 28 [0/700 (0%)]#011Loss: 0.613090\u001b[0m\n",
      "\u001b[34mTrain Epoch: 28 [600/700 (91%)]#011Loss: 0.396174\u001b[0m\n",
      "\u001b[34mTrain Epoch: 29 [0/700 (0%)]#011Loss: 0.454314\u001b[0m\n",
      "\u001b[34mTrain Epoch: 29 [600/700 (91%)]#011Loss: 0.480172\u001b[0m\n",
      "\u001b[34mTrain Epoch: 30 [0/700 (0%)]#011Loss: 0.296846\u001b[0m\n",
      "\u001b[34mTrain Epoch: 30 [600/700 (91%)]#011Loss: 0.307044\u001b[0m\n",
      "\u001b[34mTrain Epoch: 31 [0/700 (0%)]#011Loss: 0.294183\u001b[0m\n",
      "\u001b[34mTrain Epoch: 31 [600/700 (91%)]#011Loss: 0.527158\u001b[0m\n",
      "\u001b[34mTrain Epoch: 32 [0/700 (0%)]#011Loss: 0.458197\u001b[0m\n",
      "\u001b[34mTrain Epoch: 32 [600/700 (91%)]#011Loss: 0.597464\u001b[0m\n",
      "\u001b[34mTrain Epoch: 33 [0/700 (0%)]#011Loss: 0.641586\u001b[0m\n",
      "\u001b[34mTrain Epoch: 33 [600/700 (91%)]#011Loss: 0.565923\u001b[0m\n",
      "\u001b[34mTrain Epoch: 34 [0/700 (0%)]#011Loss: 0.482397\u001b[0m\n",
      "\u001b[34mTrain Epoch: 34 [600/700 (91%)]#011Loss: 0.404629\u001b[0m\n",
      "\u001b[34mTrain Epoch: 35 [0/700 (0%)]#011Loss: 0.318545\u001b[0m\n",
      "\u001b[34mTrain Epoch: 35 [600/700 (91%)]#011Loss: 0.318605\u001b[0m\n",
      "\u001b[34mTrain Epoch: 36 [0/700 (0%)]#011Loss: 0.438486\u001b[0m\n",
      "\u001b[34mTrain Epoch: 36 [600/700 (91%)]#011Loss: 0.360856\u001b[0m\n",
      "\u001b[34mTrain Epoch: 37 [0/700 (0%)]#011Loss: 0.478170\u001b[0m\n",
      "\u001b[34mTrain Epoch: 37 [600/700 (91%)]#011Loss: 0.422704\u001b[0m\n",
      "\u001b[34mTrain Epoch: 38 [0/700 (0%)]#011Loss: 0.492132\u001b[0m\n",
      "\u001b[34mTrain Epoch: 38 [600/700 (91%)]#011Loss: 0.324233\u001b[0m\n",
      "\u001b[34mTrain Epoch: 39 [0/700 (0%)]#011Loss: 0.506027\u001b[0m\n",
      "\u001b[34mTrain Epoch: 39 [600/700 (91%)]#011Loss: 0.463122\u001b[0m\n",
      "\u001b[34mTrain Epoch: 40 [0/700 (0%)]#011Loss: 0.370524\u001b[0m\n",
      "\u001b[34mTrain Epoch: 40 [600/700 (91%)]#011Loss: 0.397874\u001b[0m\n",
      "\u001b[34mTrain Epoch: 41 [0/700 (0%)]#011Loss: 0.464720\u001b[0m\n",
      "\u001b[34mTrain Epoch: 41 [600/700 (91%)]#011Loss: 0.365063\u001b[0m\n",
      "\u001b[34mTrain Epoch: 42 [0/700 (0%)]#011Loss: 0.325127\u001b[0m\n",
      "\u001b[34mTrain Epoch: 42 [600/700 (91%)]#011Loss: 0.576594\u001b[0m\n",
      "\u001b[34mTrain Epoch: 43 [0/700 (0%)]#011Loss: 0.368905\u001b[0m\n",
      "\u001b[34mTrain Epoch: 43 [600/700 (91%)]#011Loss: 0.291498\u001b[0m\n",
      "\u001b[34mTrain Epoch: 44 [0/700 (0%)]#011Loss: 0.439738\u001b[0m\n",
      "\u001b[34mTrain Epoch: 44 [600/700 (91%)]#011Loss: 0.301717\u001b[0m\n",
      "\u001b[34mTrain Epoch: 45 [0/700 (0%)]#011Loss: 0.507048\u001b[0m\n",
      "\u001b[34mTrain Epoch: 45 [600/700 (91%)]#011Loss: 0.537258\u001b[0m\n",
      "\u001b[34mTrain Epoch: 46 [0/700 (0%)]#011Loss: 0.383218\u001b[0m\n",
      "\u001b[34mTrain Epoch: 46 [600/700 (91%)]#011Loss: 0.495196\u001b[0m\n",
      "\u001b[34mTrain Epoch: 47 [0/700 (0%)]#011Loss: 0.446783\u001b[0m\n",
      "\u001b[34mTrain Epoch: 47 [600/700 (91%)]#011Loss: 0.393499\u001b[0m\n",
      "\u001b[34mTrain Epoch: 48 [0/700 (0%)]#011Loss: 0.364920\u001b[0m\n",
      "\u001b[34mTrain Epoch: 48 [600/700 (91%)]#011Loss: 0.692367\u001b[0m\n",
      "\u001b[34mTrain Epoch: 49 [0/700 (0%)]#011Loss: 0.367078\u001b[0m\n",
      "\u001b[34mTrain Epoch: 49 [600/700 (91%)]#011Loss: 0.498760\u001b[0m\n",
      "\u001b[34mTrain Epoch: 50 [0/700 (0%)]#011Loss: 0.509881\u001b[0m\n",
      "\u001b[34mTrain Epoch: 50 [600/700 (91%)]#011Loss: 0.412202\n",
      "\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.5210, Accuracy: 236/300 (79%)\n",
      "\u001b[0m\n",
      "\u001b[34m2020-08-11 15:08:13,763 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2020-08-11 15:08:21 Completed - Training job completed\n",
      "Training seconds: 77\n",
      "Billable seconds: 77\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import json\n",
    "\n",
    "hyperparameters = {\n",
    "    \"seed\": \"1\",\n",
    "    \"epochs\": 50,\n",
    "}\n",
    "\n",
    "est = sagemaker.estimator.Estimator(train_image_uri,\n",
    "                                    role,\n",
    "                                    train_instance_count=1, \n",
    "                                    #instance_type='local', # we use local mode\n",
    "                                    train_instance_type='ml.m5.xlarge',\n",
    "                                    base_job_name=prefix,\n",
    "                                    hyperparameters=hyperparameters)\n",
    "\n",
    "\n",
    "est.fit()\n",
    "\n",
    "#train_config = sagemaker.inputs.TrainingInput('s3://{0}/{1}/train/'.format(bucket, prefix), content_type='text/csv')\n",
    "#val_config = sagemaker.inputs.TrainingInput('s3://{0}/{1}/val/'.format(bucket, prefix), content_type='text/csv')\n",
    "#est.fit({'train': train_config, 'validation': val_config })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve model location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-ap-southeast-1-342474125894/pytorch-training-2020-08-11-15-05-07-606/output/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "model_location = est.model_data\n",
    "print(model_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Inference\n",
    "\n",
    "For inference, we will use default inference image. Mandatory `model_fn` is implemented in `inference.py`. PyTorchModel is used to deploy custom model that we trained previously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorchModel\n",
    "\n",
    "pytorch_model = PyTorchModel(model_data=model_location, \n",
    "                             role=role, \n",
    "                             entry_point='inference.py',\n",
    "                             source_dir='../docker/code',\n",
    "                             py_version='py3',\n",
    "                             framework_version='1.5.1',\n",
    "                            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------!"
     ]
    }
   ],
   "source": [
    "predictor = pytorch_model.deploy(initial_instance_count=1, instance_type='ml.m5.xlarge', wait=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_model.endpoint_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch.model import PyTorchPredictor\n",
    "\n",
    "endpoint_name = \"pytorch-inference-2020-08-12-08-52-57-488\"\n",
    "payload = \"1,2,3,4,5\\n2,3,4,5,6\"\n",
    "\n",
    "predictor = PyTorchPredictor(endpoint_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "payload = np.array([1,2,3,4,5])\n",
    "response = predictor.predict(payload)\n",
    "prediction = response[0].argmax(axis=1)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Optional Cleanup\n",
    "\n",
    "When you're done with the endpoint, you should clean it up.\n",
    "\n",
    "All of the training jobs, models and endpoints we created can be viewed through the SageMaker console of your AWS account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smv2",
   "language": "python",
   "name": "smv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
